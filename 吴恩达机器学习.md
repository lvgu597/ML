###　1. What is Machine Learning?

+ 定义：
  + the field of study that gives computers the ability to learn without being explicitly learned.（在进行特定编程的情况下，给予计算机学习能力的领域【Arthur Samuel】）
  + 对于某类任务 T 和性能度量 P，如果一个计算机程序在 T 上以 P 衡量的性能随着经验 E 而自我完善，那么我们称这个计算机程序在从经验 E 中学习。
+ 机器学习算法(algorithms)：
  + 监督学习(Supervised learning):
    + 教计算机如何去完成任务(we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.)
    + 给出的数据集都是“正确答案” 以生成更多 “正确答案”
      + 回归问题(regression problem)：预测**连续值**的输出[房价问题]
      + 分类问题(classification problem)：预测**离散值**输出[肿瘤问题]
  + 无监督学习(Unsupervised learning):
    + 让计算机自己学习
    + 根据给出数据集找到某种结构，将数据分成两个不同的聚类(approach problems with little or no idea what our results should look like)
      + 聚类算法[聚会两种不同的声音]
  + Octave

### 2. Linear Regression with One Variable(单变量线行回归)

+ 线性回归算法(linear regression):

  + 单变量线性回归问题：只含有一个特征/输入变量

    + 建模误差：预测的值和实际值之间的差距
    
    + 目标：使得建模误差的平方和最小的模型参数
  
+ ![image-20200704162610336](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200704162610336.png)
  
    + ![image-20200704163118742](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200704163118742.png)
    
    + 该代价函数只有一个最优解，所以局部最优解即全局最优解
    
#### 2.1 平方误差代价函数：

+ 1/2m　和　１/m最后的结果是一样的

![image-20200703154342080](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200703154342080.png)
    
+ 找minJ 即 所对应等高线图最低点
  
#### 2.2 梯度下降算法（Gradient Descent）：

+ 首先随机选择一个参数的组合，计算代价函数，然后找出下一个能让代价函数值下降最多的参数组合，持续直到找出局部最小值。

+ := 表示赋值    = 表示两边相等的真命题

+ 同步更新

![image-20200704155042857](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200704155042857.png)

+ 𝑎是学习率(learning rate)，它决定了沿着代价函数下降程度最大的方向向下迈出的步子有多大（**𝑎太小步子太小，计算量大，太大可能得不到局部最小值**），后面导数表示切线斜率，直到最低点切线斜率为0即局部最优解
+ 越接近局部最优解时 步子𝑎 越小


### 3. 矩阵和向量(Matrices and vectors)

+ 矩阵

  + 矩阵相加

  ![image-20200705120704270](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200705120704270.png)

  + 矩阵x向量

  ![image-20200705131225384](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200705131225384.png)

  + 矩阵x矩阵

  ![image-20200705141803945](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200705141803945.png)

  + 向量：只有一列的矩阵
  
  + 矩阵乘法特征
  
    + 乘法交换律  ×   （eg：维度变化）
    + 乘法结合律  √ 
  
  + 单位矩阵
  
    ![image-20200713164959733](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200713164959733.png)
  
  + 矩阵的逆(inverse)和转置(transpose)
  
    + A × *A*−1 = I
    + ![image-20200713172421123](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200713172421123.png)

### 4. Multivariate Linear Regression(多变量线性回归)

+ 单变量：房屋价格由size预测

  + ![image-20200714104936078](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200714104936078.png)

+ 多变量：房屋价格由size、卧室数量、楼层、房龄预测

  + ![image-20200714104840014](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200714104840014.png)
  + ![image-20200715171002206](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200715171002206.png)

#### 4.1 特征缩放(Feature Scaling)
+ xi  approximately [-1,1] range

#### 4.2 均值归一化(Mean normalization)
+ ![image-20200714152715336](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200714152715336.png)

+ Where μi is the **average** of all the values for feature (i) and si is the range of values (max - min), or si is the standard deviation（标准差）.

#### 4.3 学习率
+ *α*太小：收敛慢
+ *α*太大：可能并不是每次迭代代价函数都减少并且可能不会收敛

#### 4.4 特征和多项式回归

+ 选择不同的特征：房屋宽、房屋深度、房屋面积等
+ 选择不同的多项式：二次、三次、更号等等，不同特征、多项式对应不同的算法模型
  + ![image-20200714212043841](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200714212043841.png)
    + if x1 ∈ [1,1000] then x1²∈[1,1000000]
  + ![image-20200714212021478](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200714212021478.png)

#### 4.5 正规方程（区别于迭代法的直接解法）

+ 求解代价函数最小值的点：导函数为0的点
+ ![image-20200715155101740](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200715155101740.png)

+ 梯度下降算法和正规方程算法优缺点分析：

  |             梯度下降             |              正规方程              |
  | :------------------------------: | :--------------------------------: |
  |        需要选择学习律*α*         |             不需要*α*              |
  |           需要多次迭代           |             不需要迭代             |
  | 即使特征值很多也没关系（O(kn²)） | 特征值越多计算越慢(O(n³)、n<10000) |

##### 4.5.1 矩阵不可逆时正规方程解法

+ 不可逆原因：
  + 两个特征值线性相关（eg：一个是另一个倍数）
  + 数据集 m <= 特征值数量n 

### 5. Octave

#### 5.1 基本操作

+ ones(m,n)  %生成一个mxn的全1矩阵

```octave
octave:1> ones(2,3)
ans =

   1   1   1
   1   1   1
```

+ hist(a) %画出a的直方图
+ eye(4) %生成一个4x4的单位矩阵

```octave
octave:4> eye(4)
ans =

Diagonal Matrix

   1   0   0   0
   0   1   0   0
   0   0   1   0
   0   0   0   1
```

+ size()  %返回变量规模

```octave
octave:8> size(a)
ans =

   2   3
```

+ length()  %返回最大的维度

```octave
octave:9> length(a)
ans =  3
```

+ help help%显示函数详细信息

#### 5.2数据处理

+ pwd ls
+ load 路径  %加载数据，变量名即为文件名
+ who %显示当前内存创建的一些变量  whos %显示更详细的信息

```octave
octave:10> who
Variables in the current scope:

a    ans
```

+ clear(a)  %删除变量a
+ save hello.dat a   %将a存入hello.dat

#### 5.3 基本运算

+ A .* B %对应元素相乘，规模不同会报错
+  A .^ 2 %所有元素平方
+ A + 1 %所有元素+1
+ A' %A 的转置
+ sum(A)  %计算和  prod(A) %计算乘积   floor(A) %向下取整 ceil(A) %向上取整
+ filpup(A) %翻转

#### 5.4 可视化

+ plot(m,n)  %绘制sin函数

+ print -dpng 'myplot.png' %保存图片
+ axis([0 1 0 1]) %设置横纵坐标
+ , %连接  ，多条命令同时使用

#### 5.5 流程语句

+ for循环

```octave
octave:2> for i=1:10,
> v(i) += 2;
> end
```

+ while 循环

```octave
octave:4> i = 1;
octave:5> while i < 5,
> i++;
> end
```

+ 函数

  + 创建一个.m文件

  ```octave
  function y = f(x)  %y为返回值，x为形参  [x1,x2] 返回多个值
  y = x^2;
  ```

  + 在当前目录下可以直接使用函数 f(x) 

+ addpath('') %添加某个指定路径

+ submit 提交

### 6. Logistic回归

#### 6.1 分类

+ 二分类：0或1

  + 邮箱分类
  + 肿瘤判定

+ 回归算法并不适用与分类模型

  ![image-20200727171813330](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200727171813330.png)

#### 6.2 函数模型

+ ![image-20200727223739473](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200727223739473.png)

+ g(z) 为sigmoid函数

  + 函数图像：

    ![image-20200727224005488](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200727224005488.png)
  
  + g(z) >= 0.5 即 z > 0

#### 6.3 代价函数    

+ 代价函数：用于找到最优解的目的函数（映射）

  ![image-20200806165116815](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200806165116815.png)

+ 如果正确答案是ｙ＝１即有肿瘤，那么 如果我们预测值h=0，则代价函数cost=1，如果预测值h=1，则代价函数 趋于无穷大

  ![image-20200806170108487](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200806170108487.png)

+ 合并成一个后代价函数为：

  Cost(*hθ*(*x*),*y*)=−*y*log(*hθ*(*x*))−(1−*y*)log(1−*hθ*(*x*))

+ 完整版代价函数，且图像为凸函数：
  ![image-20200806171053273](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200806171053273.png)

#### 6.4 高级优化算法

+ 一些优化代价函数的算法
  + 梯度下降算法(Gradient )   --计算导数
  + Conjugate gradiebt
  + BFGS
  + L-BFGS
+ 高级优化算法优缺点
  + 优点：不需要手动选择下降率α、比梯度下降算法收敛得更快
  + 缺点：更复杂
+ 可以通过使用一些库函数来实现

#### 6.5 多分类问题：一对多

+ 转换为多个独立的二元分类问题

### 7. 正则化

#### 7.1 过拟合问题

+ 欠拟合(underfitting)：无法很好拟合数据
  + 变量比较少
+ 过拟合(overfitting)：用更高阶多项式来拟合数据，虽然能拟合基本所有数据，但是太庞大，变量太多，没有足够的数据来约束
  + 多变量
  + 代价函数≈0
  + 解决方法:
    + 减小选取变量的数量（人工/利用算法进行筛选）
    + 正则化
      + 保留所有特征，但减小量级或者 $θ_j$的大小
      + Regularization works well when we have a lot of slightly useful features.

#### 7.2 线性回归的正则化

+ 正则化

  + 减小 θ_j的大小，θ\_j无限小时该项无限趋近于0

+ 引入正则化参数(regularization parameter)  λ

  + 引入后代价函数:

  ![image-20200810170759037](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200810170759037.png)

  + **当λ过大时，$θ_1$、$θ_2$、$θ_3$...趋近于0 **

+ 梯度下降函数正则化

  ![image-20200810172317344](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200810172317344.png)

  + 注意;θ\_0不加入正则化，所以梯度下降函数正则化从θ\_1开始
  + 相当于每次更新θ\_j 都先*一个<1 的数，再更新 

+ 正规方程法正则化

  ![image-20200810173641858](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200810173641858.png)

  + 当λ > 0 时，括号内一定可逆，即为非奇异矩阵

#### 7.3 logistic回归的正则化

+ 梯度下降函数正则化

  ![image-20200810175927049](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200810175927049.png)

  + 注意：j从1开始

### 8. 神经网络：表述

#### 8.1 非线性回归

+ 对于特征很多的问题，线性回归计算量大(考虑不同特征组合)且更容易出现过拟合情况

#### 8.2 神经网络模型

+ ![image-20200812153532243](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20200812153532243.png)
  + 第一层为输入层	最后一层为输出层	中间一层为隐藏层
  + 𝑎𝑖 (𝑗) 代表第𝑗 层的第 𝑖 个激活单元。 𝜃(𝑗)代表从第 𝑗 层映射到第𝑗 + 1 层时的权重的矩阵
  + 把𝑥, 𝜃, 𝑎 分别用矩阵表示，我们可以得到𝜃 ⋅ 𝑋 = 𝑎 

### 9. 神经网络的学习

#### 9.1 代价函数 

+ 基本术语：

  + L=总层数
  + s_l = 第l层的单元数（不包括偏差单元）
  + K = 输出单元数目（二分类中K=1） 

+ 代价函数

  + 逻辑回归代价函数

  ![image-20200924155317186](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200924155317186.png)

  + 神经网络代价函数（相当于换成K分类，K为1时即为二分类）

  ![image-20200924155422910](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200924155422910.png)

#### 9.2 反向传播算法(Backpropagation Algorithm)

+ 用![image-20200924171251122](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200924171251122.png)表示第l层第j个节点的 误差（error）
+ 反向传播算法即从最后一层的值（确定eg：[1 0 0 0]T）倒推前面的值
+ 反向传播函数其实是在计算![image-20200924171251122](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200924171251122.png)的值，该值本质上即为代价函数cost(i)关于z的偏导
  + ![image-20200924172359205](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200924172359205.png)
+ 反向传播和正向传播计算方法及其相识，同样是利用权重，不过将x换成![image-20200924171251122](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200924171251122.png)
  + ![image-20200924172514485](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200924172514485.png)
  + ![image-20200924172454411](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200924172454411.png)
  + 注意红色和粉色的线，这里的反向传播函数都不包括偏置单元，实际偏置单元并不影响最终结果（偏置单元相当于常数项）

#### 9.3 梯度检测

+ 利用图形结合的方法，**检测计算得到的值和生成的DVec是否大致相等**

  ![image-20200924194213385](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200924194213385.png)

+ 拓展到向量

  ![image-20200924194400393](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200924194400393.png)

+ 该方法计算量大并且很慢，所以最终使用的还是前面的反向传播算法

#### 9.4 随即初始化

+ 神经网络中设置初始$\theta$为zeros(n,1) 时，起不到任何作用
  
+ 相当于layer1的weight全为0
  
+ 实际初始化为[-$\theta$, $\theta$]之间

  ![image-20200924200256492](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200924200256492.png)

  + 正常思路：
    + 初始化为[-$\theta$，$\theta$]之间的数
    + 使用反向传播算法
    + 使用梯度检测
    + 使用优化算法优化

#### 9.5 神经网络完整流程

+ 选择一个网络结构：有多少层以及每层多少隐藏单元

  + 输入单元个数 = 训练集的特征数量
  + 输出单元个数 = 分类数（k，且转成向量的方式[1 0 0 0]T 、[0 1 0 0]T）
  + 如果隐藏层数大于1，确保每个隐藏层的单元数相同，通常情况下隐藏层单元的个数越多越好
  + 通常我们需要决定隐藏层的层数和每个中间层的单元数

+ 训练一个神经网络：

  1、[$\theta$, $\theta$]

  2、利用正向传播计算所有的ℎ𝜃(𝑥) 

  3、编写计算代价函数 𝐽 的代码

  4、利用反向传播函数计算所有偏导数

  5、利用数值检验方法检验这些偏导数

  6、利用优化算法（eg：梯度下降算法）来最小化代价函数（𝐽 为非凸函数，所以理论上可能收敛与局部最小值）

### 10. 应用机器学习的例子

#### 10.1 决定下一步做什么

+ 常见优化模型的方法
  + 尝试减少特征的数量
  + 尝试获得更多的特征
  + 尝试增加更多多项式（x1²，x2²，x1x2...）
  + 尝试减少正则化程度𝜆 
  + 尝试增加正则化程度𝜆

#### 10.2 算法评估方法

+ 将原始数据随机分为7:3（训练集:测试集）

+ 有一下两种计算误差的方法

  + 对于线性回归模型，可以直接利用计算出来的theater 来算出test集的代价函数

  + 对于逻辑回归模型，出来可以用上面的方法

    ![image-20200925174941962](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200925174941962.png)

    + 还可以使用**错误分类**的方法：
      + 对于每个测试集实例，通过计算![image-20200925175116743](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200925175116743.png)然后对结果求平均

#### 10.3 模型选择和交叉验证集

+ 使用60%的数据作为训练集，使用20%的数据作为交叉验证集，使用20%的数据作为测试集

+ 模型选择的方法：

  + 使用训练集训练出10个模型

  + 用10个模型分别对交叉验证集计算得出的交叉验证误差（代价函数的值）

  + 选取代价函数最小的模型

  + 用第三个步骤选出的模型对测试集计算得出推广误差（代价函数的值）

    ![image-20200926122648329](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200926122648329.png)

  + 训练代价误差<img src="C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200928203421154.png" alt="image-20200928203421154" style="zoom:60%;" />、交叉代价误差<img src="C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200928203514658.png" alt="image-20200928203514658" style="zoom:60%;" />和 d 之间的关系

  ![image-20200928203615659](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200928203615659.png)

+ 

#### 10.4 诊断偏差和方差（欠拟合/过拟合问题）

+ 偏差比较大（欠拟合、d小）VS 方差比较大（过拟合、d大）
+ **偏差/欠拟合/d小**
  + 训练集和交叉验证集误差相近且都比较大
+ **偏差/过拟合/d大**
  + 训练集误差比较小、交叉验证集误差比较大且交叉验证集误差远大于训练集误差

#### 10.5 正则化对偏差、方差的影响

+ 正则化：

  +  即最后加一项 $\frac{λ}{2m}\sum_{j=1}^m\theta_j^2$

+  $\lambda$过大时，$\theta_1、\theta_2...\approx$0 ，此时欠拟合

+  $\lambda$ = 0时，相当于无正则化，对原模型无影响，此时过拟合 

+  正确选择$\lambda$的方法为：

   1、根据倍增法选取十二个$\lambda$ 的值，及[0，0.01,0.02,0.04..10]，并训练出12个不同程度正则化的模型

   2、用这12个模型分别对交叉验证集计算出交叉验证误差

   3、选择得出较交叉验证误差最小的模型

   4、运用步骤3中选出的模型对测试集计算得出推广误差，相应图表如下

   ![image-20200929201048288](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200929201048288.png)

   + 当 𝜆 较小时，训练集误差较小（过拟合）而交叉验证集误差较大 
   + 随着 𝜆 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后
     增加


#### 10.6 绘制学习曲线（检查学习算法是否正常/改进算法的表现）

+ 当出现如下欠拟合情况时，无论如何增大训练集都不会有太大改观

  ![image-20200929205910987](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200929205910987.png)

+ 当出现如下过拟合/高方差情况时，增加训练集可以提高模型效果

  ![image-20200929210027045](C:\Users\11361\AppData\Roaming\Typora\typora-user-images\image-20200929210027045.png)

#### 10.7 总结：决定下一步做什么（理解清楚！）

  + 获得更多训练事例→解决**高方差/过拟合** （本身欠拟合的话增加更多事例依然欠拟合）
  + 尝试减少训练特征→解决**高方差/过拟合** （相当于减少x项）
  + 尝试获得更多特征→解决**高偏差/欠拟合** （相当于增加x项）
  + 尝试增加多项式（$x_1^2、x_2^2、x_1x_2...$）→解决**高偏差/欠拟合** 
  + 尝试减小正则化程度$\lambda$ → 解决**高偏差/欠拟合** （$\lambda$越趋近于0，正则化项越小，对原来影响越小）
  + 尝试增加正则化程度$\lambda$ → 解决**高方差/过拟合**  （$\lambda$越大，正则化项越大，对原来影响越大）

### 11. 机器学习系统设计（设计一个垃圾邮件分类器）

#### 11.1 确定执行的优先级

+ 可以选择一个由100个垃圾邮件中的词所构成的列表，根据这些词是否在邮件中出现来获取特征向量，下一步可以选择以下方向：
  + 收集更多数据（比如说“honeypot”项目集，但这种方法并不总是有效的）
  + 找出一些更复杂的特征（比如说使用邮箱头数据）
  + 使用一些算法来处理输入（检查一些刻意拼写的错误）

#### 11.2 误差分析

+ 构建一个学习算法的推荐方法为：

  + 从一个简单的算法开始，快速实现并且用交叉验证集数据测试这个算法

  + 绘制学习曲线，从而决定是增加数据，或则添加更多特征还是其他的一些选择
  + 进行误差分析：人工检查**交叉验证集**中产生预测误差的实例，观察这些实例是否存在某种系统化的趋势

#### 11.3 偏斜类（skewed classes）的误差度量

+ 偏斜类问题：训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例

+ 查准率（Precision）和召回率（Recall）

+ 将算法预测值和实际值的结果分为四种情况

  | 预测↓    实际→ |           1           |           0           |
  | :------------: | :-------------------: | :-------------------: |
  |       1        |     True Positive     | False Positive（假1） |
  |       0        | False Negative（假0） |     True Negative     |

+ 查准率（Pricision） =  $\frac{True Positive}{True Positive+False Positive}$  （所有检测没得病的病人 中确实没得病的比例）

+ 召回率（recall）= $\frac{TrueNegative}{FlaseNegative+TrueNegative}$ （所有检测得病的病人中 确实得病的比例）

#### 11.4 精确度和召回率（Recall）的权衡

+ 可以通过提高$h_\theta$的值来提高查准率，此时召回率会降低（想要更准确得告诉病人得了癌症）
+ 可以通过降低$h_\theta$的值来提高召回率，此时查准率会降低（想要更准确得让病人去治疗，即使他没得癌症）

